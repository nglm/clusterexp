{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Courtesy to https://github.com/deric/clustering-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pycvi.cluster import generate_all_clusterings\n",
    "from pycvi.cvi import Inertia, GapStatistic, ScoreFunction, Hartigan, Diameter, CalinskiHarabasz, Silhouette, CVIs\n",
    "from pycvi.compute_scores import compute_all_scores\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import (\n",
    "    arff_from_github, load_data_from_github, get_data_labels ,URL_ROOT,\n",
    "    write_list_datasets, UNIMODAL, UNLABELED, INVALID, UNKNOWN_K,\n",
    "    print_heads, TOO_MANY_LABELS, TOO_MANY_SAMPLES, INVALID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"artificial\"\n",
    "#DATA_SOURCE = \"real-world\"\n",
    "PATH = f\"{URL_ROOT}{DATA_SOURCE}/\"\n",
    "RES_DIR = f'../res/{DATA_SOURCE}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mopsi-finland.arff',\n",
       " 'long3.arff',\n",
       " 'zelnik1.arff',\n",
       " 'insect.arff',\n",
       " 'disk-4500n.arff',\n",
       " 'disk-4000n.arff',\n",
       " 'cure-t0-2000n-2D.arff',\n",
       " 'circle.arff',\n",
       " 'zelnik6.arff',\n",
       " 'elly-2d10c13s.arff',\n",
       " 'zelnik3.arff',\n",
       " 'disk-4600n.arff',\n",
       " 'aggregation.arff',\n",
       " 'donutcurves.arff',\n",
       " 'dartboard1.arff',\n",
       " 's-set3.arff',\n",
       " 'disk-1000n.arff',\n",
       " 'complex8.arff',\n",
       " 'ds4c2sc8.arff',\n",
       " 'square4.arff',\n",
       " 'square5.arff',\n",
       " 'dpb.arff',\n",
       " 'dense-disk-3000.arff',\n",
       " 'dpc.arff',\n",
       " 'DS-577.arff',\n",
       " 'cluto-t5-8k.arff',\n",
       " 'jain.arff',\n",
       " '2dnormals.arff',\n",
       " 'target.arff',\n",
       " 'cluto-t4-8k.arff',\n",
       " 'pmf.arff',\n",
       " 'blobs.arff',\n",
       " 'banana.arff',\n",
       " 'sizes2.arff',\n",
       " '2d-3c-no123.arff',\n",
       " 'sizes5.arff',\n",
       " 'pathbased.arff',\n",
       " 'cure-t2-4k.arff',\n",
       " 'triangle2.arff',\n",
       " 'square2.arff',\n",
       " 'hypercube.arff',\n",
       " '2d-4c-no9.arff',\n",
       " 'twenty.arff',\n",
       " 'hepta.arff',\n",
       " 'smile3.arff',\n",
       " 'spherical_4_3.arff',\n",
       " 'golfball.arff',\n",
       " 'dense-disk-5000.arff',\n",
       " 'twodiamonds.arff',\n",
       " 'birch-rg3.arff',\n",
       " 'donut2.arff',\n",
       " 'cuboids.arff',\n",
       " 'elliptical_10_2.arff',\n",
       " 'sizes3.arff',\n",
       " 'D31.arff',\n",
       " 'compound.arff',\n",
       " 'long1.arff',\n",
       " 'long2.arff',\n",
       " 'curves2.arff',\n",
       " 's-set4.arff',\n",
       " 'disk-5000n.arff',\n",
       " 'donut1.arff',\n",
       " '2d-10c.arff',\n",
       " 'threenorm.arff',\n",
       " '3-spiral.arff',\n",
       " 'simplex.arff',\n",
       " 'cassini.arff',\n",
       " 'sizes4.arff',\n",
       " 'complex9.arff',\n",
       " 'dartboard2.arff',\n",
       " 'sizes1.arff',\n",
       " 'zelnik2.arff',\n",
       " 'smile2.arff',\n",
       " '2d-20c-no0.arff',\n",
       " 'spherical_5_2.arff',\n",
       " 'DS-850.arff',\n",
       " 'disk-6000n.arff',\n",
       " 'donut3.arff',\n",
       " 'fourty.arff',\n",
       " '2sp2glob.arff',\n",
       " 'disk-3000n.arff',\n",
       " 'lsun.arff',\n",
       " 'diamond9.arff',\n",
       " 'zelnik4.arff',\n",
       " '2d-4c-no4.arff',\n",
       " 'st900.arff',\n",
       " 'birch-rg2.arff',\n",
       " 'shapes.arff',\n",
       " 'ds2c2sc13.arff',\n",
       " '3MC.arff',\n",
       " 'cluto-t7-10k.arff',\n",
       " 'birch-rg1.arff',\n",
       " 'square3.arff',\n",
       " 'atom.arff',\n",
       " 'curves1.arff',\n",
       " 'spiralsquare.arff',\n",
       " 'ds3c3sc6.arff',\n",
       " 'cure-t1-2000n-2D.arff',\n",
       " 'xclara.arff',\n",
       " 'cluto-t8-8k.arff',\n",
       " 'aml28.arff',\n",
       " 'zelnik5.arff',\n",
       " 'square1.arff',\n",
       " 'gaussians1.arff',\n",
       " 's-set1.arff',\n",
       " 'spherical_6_2.arff',\n",
       " 'spiral.arff',\n",
       " 'triangle1.arff',\n",
       " 'rings.arff',\n",
       " 'mopsi-joensuu.arff',\n",
       " 's-set2.arff',\n",
       " 'engytime.arff',\n",
       " 'flame.arff',\n",
       " '2d-4c.arff',\n",
       " 'longsquare.arff',\n",
       " 'smile1.arff',\n",
       " 'impossible.arff',\n",
       " 'wingnut.arff',\n",
       " 'R15.arff',\n",
       " 'xor.arff',\n",
       " 'tetra.arff',\n",
       " 'chainlink.arff']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = f'{URL_ROOT}{DATA_SOURCE}.txt'\n",
    "\n",
    "all_datasets = []\n",
    "for line in urllib.request.urlopen(fname):\n",
    "    all_datasets.append(line.decode('utf-8').strip())\n",
    "\n",
    "print(len(all_datasets))\n",
    "all_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save list and headers of all original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fname = f'{RES_DIR}all_datasets-{DATA_SOURCE}.txt'\n",
    "#write_list_datasets(list_fname, all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_heads(fnames=all_datasets, path=PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containing multimodal and labeled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['long3.arff',\n",
       " 'zelnik1.arff',\n",
       " 'insect.arff',\n",
       " 'disk-4500n.arff',\n",
       " 'disk-4000n.arff',\n",
       " 'cure-t0-2000n-2D.arff',\n",
       " 'circle.arff',\n",
       " 'zelnik6.arff',\n",
       " 'elly-2d10c13s.arff',\n",
       " 'zelnik3.arff',\n",
       " 'disk-4600n.arff',\n",
       " 'aggregation.arff',\n",
       " 'donutcurves.arff',\n",
       " 'dartboard1.arff',\n",
       " 'disk-1000n.arff',\n",
       " 'complex8.arff',\n",
       " 'ds4c2sc8.arff',\n",
       " 'square4.arff',\n",
       " 'square5.arff',\n",
       " 'dpb.arff',\n",
       " 'dense-disk-3000.arff',\n",
       " 'dpc.arff',\n",
       " 'DS-577.arff',\n",
       " 'cluto-t5-8k.arff',\n",
       " 'jain.arff',\n",
       " '2dnormals.arff',\n",
       " 'target.arff',\n",
       " 'cluto-t4-8k.arff',\n",
       " 'pmf.arff',\n",
       " 'blobs.arff',\n",
       " 'banana.arff',\n",
       " 'sizes2.arff',\n",
       " '2d-3c-no123.arff',\n",
       " 'sizes5.arff',\n",
       " 'pathbased.arff',\n",
       " 'cure-t2-4k.arff',\n",
       " 'triangle2.arff',\n",
       " 'square2.arff',\n",
       " 'hypercube.arff',\n",
       " '2d-4c-no9.arff',\n",
       " 'twenty.arff',\n",
       " 'hepta.arff',\n",
       " 'smile3.arff',\n",
       " 'spherical_4_3.arff',\n",
       " 'dense-disk-5000.arff',\n",
       " 'twodiamonds.arff',\n",
       " 'donut2.arff',\n",
       " 'cuboids.arff',\n",
       " 'elliptical_10_2.arff',\n",
       " 'sizes3.arff',\n",
       " 'D31.arff',\n",
       " 'compound.arff',\n",
       " 'long1.arff',\n",
       " 'long2.arff',\n",
       " 'curves2.arff',\n",
       " 'disk-5000n.arff',\n",
       " 'donut1.arff',\n",
       " '2d-10c.arff',\n",
       " 'threenorm.arff',\n",
       " '3-spiral.arff',\n",
       " 'simplex.arff',\n",
       " 'cassini.arff',\n",
       " 'sizes4.arff',\n",
       " 'complex9.arff',\n",
       " 'dartboard2.arff',\n",
       " 'sizes1.arff',\n",
       " 'zelnik2.arff',\n",
       " 'smile2.arff',\n",
       " '2d-20c-no0.arff',\n",
       " 'spherical_5_2.arff',\n",
       " 'DS-850.arff',\n",
       " 'disk-6000n.arff',\n",
       " 'donut3.arff',\n",
       " 'fourty.arff',\n",
       " '2sp2glob.arff',\n",
       " 'disk-3000n.arff',\n",
       " 'lsun.arff',\n",
       " 'diamond9.arff',\n",
       " 'zelnik4.arff',\n",
       " '2d-4c-no4.arff',\n",
       " 'st900.arff',\n",
       " 'shapes.arff',\n",
       " 'ds2c2sc13.arff',\n",
       " '3MC.arff',\n",
       " 'cluto-t7-10k.arff',\n",
       " 'square3.arff',\n",
       " 'atom.arff',\n",
       " 'curves1.arff',\n",
       " 'spiralsquare.arff',\n",
       " 'ds3c3sc6.arff',\n",
       " 'cure-t1-2000n-2D.arff',\n",
       " 'xclara.arff',\n",
       " 'cluto-t8-8k.arff',\n",
       " 'aml28.arff',\n",
       " 'zelnik5.arff',\n",
       " 'square1.arff',\n",
       " 'gaussians1.arff',\n",
       " 's-set1.arff',\n",
       " 'spherical_6_2.arff',\n",
       " 'spiral.arff',\n",
       " 'triangle1.arff',\n",
       " 'rings.arff',\n",
       " 's-set2.arff',\n",
       " 'engytime.arff',\n",
       " 'flame.arff',\n",
       " '2d-4c.arff',\n",
       " 'longsquare.arff',\n",
       " 'smile1.arff',\n",
       " 'impossible.arff',\n",
       " 'wingnut.arff',\n",
       " 'R15.arff',\n",
       " 'xor.arff',\n",
       " 'tetra.arff',\n",
       " 'chainlink.arff']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [\n",
    "    fname for fname in all_datasets\n",
    "    if (fname not in UNIMODAL+UNLABELED)]\n",
    "print(len(filenames))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_heads(fnames=filenames, path=PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save list of datasets that are suitable for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_for_exp = list(set(UNKNOWN_K+TOO_MANY_LABELS+TOO_MANY_SAMPLES+INVALID))\n",
    "fname_exp_theory = [\n",
    "    fname for fname in all_datasets\n",
    "    if (fname not in not_for_exp)]\n",
    "print(len(fname_exp_theory))\n",
    "fname_exp_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_theory_list_fname = f'{RES_DIR}datasets_experiments_theory-{DATA_SOURCE}.txt'\n",
    "#write_list_datasets(exp_theory_list_fname, fname_exp_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = [f for f in all_datasets if f in not_for_exp]\n",
    "excluded_list_fname = f'{RES_DIR}datasets_excluded-{DATA_SOURCE}.txt'\n",
    "#write_list_datasets(excluded_list_fname, excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"zelnik1\", \"target\", \"long1\", \"xclara\", \"banana\", \"diamond9\"]\n",
    "\n",
    "for d in dataset_names:\n",
    "    data, labels, n_labels, meta = get_data_labels(\n",
    "        f\"{URL_ROOT}artificial/{d}.arff\", path=\"\"\n",
    "    )\n",
    "    # labels = labels.astype(float)\n",
    "    pd.DataFrame(labels).to_csv(f\"./{d}_labels.csv\", header=False, index=False)\n",
    "    pd.DataFrame(data).to_csv(f\"./{d}_data.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = arff_from_github(f'{URL_ROOT}artificial/diamond9.arff')\n",
    "df = pd.DataFrame(data)\n",
    "df.plot.scatter(\"x\", \"y\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "[t == \"int\" for t in df.dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_old(data, clusterings, titles):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=4, sharex=True, sharey=True, figsize=(15,10),\n",
    "        tight_layout=True\n",
    "    )\n",
    "    for i, ax in enumerate(axes.flat[:len(clusterings)]):\n",
    "        # Plot the clustering selected by a given score\n",
    "        for i_label, cluster in enumerate(clusterings[i]):\n",
    "            ax.scatter(data[cluster, 0], data[cluster, 1], s=0.5)\n",
    "        ax.set_title(str(titles[i]))\n",
    "    return fig, ax\n",
    "\n",
    "def plot_clusters(data, clusterings, titles):\n",
    "    # Some datasets are in 3D\n",
    "    (N, d) = data.shape\n",
    "    if d == 2:\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=2, ncols=4, sharex=True, sharey=True, figsize=(15,10),\n",
    "            tight_layout=True\n",
    "        )\n",
    "    elif d == 3:\n",
    "        fig = plt.figure(figsize=(15,10), tight_layout=True)\n",
    "    # Plot the clustering selected by a given score\n",
    "    for i in range(len(clusterings)):\n",
    "        # Plot clusters one by one\n",
    "        if d == 2:\n",
    "            ax = axes.flat[i]\n",
    "        elif d == 3:\n",
    "            ax = fig.add_subplot(2, 4, i+1, projection='3d')\n",
    "        for i_label, cluster in enumerate(clusterings[i]):\n",
    "            if d == 2:\n",
    "                ax.scatter(data[cluster, 0], data[cluster, 1], s=0.5)\n",
    "            elif d == 3:\n",
    "                ax.scatter(\n",
    "                    data[cluster, 0], data[cluster, 1], data[cluster, 2], s=0.5\n",
    "                )\n",
    "        ax.set_title(str(titles[i]))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis = df.iloc[:, 0:-1]\n",
    "df_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_bis.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_range = [i for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_data = []\n",
    "l_n_labels = []\n",
    "l_fname = [\n",
    "    # \"diamond9.arff\",\n",
    "    \"tetra.arff\"\n",
    "    # \"xclara.arff\",\n",
    "    # \"birch-rg1.arff\",\n",
    "    # \"golfball.arff\",\n",
    "]\n",
    "for fname in l_fname:\n",
    "    if fname in UNLABELED:\n",
    "        with_labels = False\n",
    "        n_labels = 1\n",
    "    else:\n",
    "        with_labels = True\n",
    "    data, labels, meta = load_data_from_github(\n",
    "        PATH + fname, with_labels=with_labels\n",
    "    )\n",
    "    if with_labels:\n",
    "        n_labels = len(np.unique(labels))\n",
    "    l_data.append(data)\n",
    "    l_n_labels.append(n_labels)\n",
    "    print(len(data), n_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(X):\n",
    "    N = len(X)\n",
    "\n",
    "    clusterings = generate_all_clusterings(\n",
    "            X,\n",
    "            AgglomerativeClustering,\n",
    "            n_clusters_range,\n",
    "            DTW=False,\n",
    "            scaler=StandardScaler(),\n",
    "        )\n",
    "    selected_clusterings = []\n",
    "\n",
    "    for s in CVIs:\n",
    "        score = s()\n",
    "        print(\" ================ {} ================ \".format(str(score)))\n",
    "        if N > 10000 and s in [GapStatistic, Silhouette]:\n",
    "            print(\"Dataset too big for {}\".format(score))\n",
    "        else:\n",
    "            scores = compute_all_scores(\n",
    "                score,\n",
    "                X,\n",
    "                clusterings,\n",
    "                DTW=False,\n",
    "                scaler=StandardScaler(),\n",
    "            )\n",
    "\n",
    "            for k in n_clusters_range:\n",
    "                print(k, scores[0][k])\n",
    "\n",
    "            selected_k = score.select(scores)[0]\n",
    "            selected_clusterings.append(clusterings[0][selected_k])\n",
    "            print(\"Selected k {}\".format(selected_k))\n",
    "\n",
    "    fig = plot_clusters(X, selected_clusterings, CVIs)\n",
    "    fig.savefig(\"./tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, X in enumerate(l_data):\n",
    "    print(\" ---------------- DATASET {} ---------------- \".format(l_fname[i]))\n",
    "    print(\" --------------------- True k: {} --------------------- \".format(l_n_labels[i]))\n",
    "    experiment(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clusterexp-s4OGTNKW-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
